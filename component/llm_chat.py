import time
import copy
import openai
import requests
import os
import streamlit as st

from myutils.config import DOCQA_PROMPT, CHAT_EXAMPLES, BAICHUAN_URL, audit_PROMPT
from myutils.es import ElasticsearchServer
from myutils.faiss import FaissDocServer
from myutils.process_data import loadtxt

from .utils import handle_response, create_message

content, labels, backends = loadtxt()
result = {}
for item in content:
    if item[0] in result:
        result[item[0]] += "{}çš„{}ä¸º{}".format(item[0], item[1], item[2])
    else:
        result[item[0]] = "{}çš„{}ä¸º{}".format(item[0], item[1], item[2])
result['å…¶ä»–'] = ''


def llm_base():
    with st.sidebar:
        # æ¨¡å‹å‚æ•°é€‰æ‹©
        temperature = st.slider("Temperatureï¼š", 0.0, 1.0, 0.7, 0.05)
        history_len = st.number_input("å†å²å¯¹è¯è½®æ•°ï¼š", 0, 10, 1)

        # æ¸…ç©ºå¯¹è¯
        cols = st.columns(2)

        if cols[1].button(
                "æ¸…ç©ºå¯¹è¯",
                use_container_width=True,
        ):
            st.session_state.messages = []
            st.experimental_rerun()

    # Display chat messages from history on app rerun
    st.title("ğŸ’¬ä»£ç éšè¡Œ")
    chat_input_placeholder = "è¯·è¾“å…¥å¯¹è¯å†…å®¹ï¼Œæ¢è¡Œè¯·ä½¿ç”¨Shift+Enter "

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input(chat_input_placeholder, key="prompt"):
        full_response = ''
        with st.chat_message("user"):
            st.markdown(prompt)

        st.session_state.messages.append(create_message("user", prompt))
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            tmp = copy.deepcopy(st.session_state.messages)
            for i in reversed(range(len(tmp))):
                if tmp[i]['role'] == 'user':
                    tmp[i][
                        'content'] = f"ä½ æ˜¯ç”±å—äº¬å®¡è®¡å¤§å­¦æ™ºèƒ½å®¡è®¡å›¢é˜Ÿç ”å‘çš„â€˜å®¡å…ƒâ€™å¤§æ¨¡å‹ï¼Œç›®å‰è¿˜åœ¨ä¸æ–­å®Œå–„ä¸­ã€‚\n å¦‚æœä¸æ˜¯è¯¢é—®èº«ä»½ä¿¡æ¯å°±æ­£å¸¸å›ç­”ã€‚\n <<é—®é¢˜>>ï¼š {tmp[i]['content']}"
                    break
            full_response = handle_response(tmp, temperature, history_len,
                                            message_placeholder)
            st.session_state.messages.append(create_message("assistant", full_response))


